# -*- coding: utf-8 -*-
"""
Created on Wed Aug 12 20:36:26 2020

Working on developing a deep learning chatbot in python with TensorFlow.

Thinking about this.  Create a spider to crawl specific websites, then use 
the pulled text to create a specialized chatbot.  Matrix Movie trained chatbot,
here we come?

Link to reddit data: https://files.pushshift.io/reddit/comments/

@author: Levitannin
"""


import sqlite3
import json
from datetime import datetime

timeframe = '2015-05' # Comment timeframe for review.

sql_transaction = []

connection = sqlite3.connect('{}.db'.format(timeframe))
cursor = connection.cursor()



def create_table():
    #   unix INT == time returned as an int for UTC
    #   specialize based on subreddits -- cybersecurity based on cybersecurity subs, example
    cursor.execute("""CREATE TABLE IF NOT EXISTS 
                   parent_reply(parent_id TEXT PRIMARY KEY, comment_id TEXT UNIQUE,
                   parent TEXT, comment TEXT, subreddit TEXT, unix INT, score INT""")

def format_data(data):
    #   Prepping for Tokenizing the text.
    data = data.replace("\n", "  newlinechar  ").replace("\r", "  newlinechar  ").replace('"', "'")
    return data

def find_parent(pid):
    sql = "SELECT comment FROM parent_reply WHERE comment_id = '{}' LIMIT 1".format(pid)

if __name__ == "__main__":
    create_table()
    
    row_counter = 0 #   how many rows have we gone through thusfar?
    paired_rows = 0 #   How many parent--child pairs have we found? Reddit specific
    
    with open("FILE STORAGE PATH:/dir/dir/{}/RC_{}".format(timeframe.split('-')[0], timeframe), buffering = 1000) as f:
        for row in f:
            row_counter += 1
            row = json.loads(row)
            parent_id = row['parent_id']
            body = format_data(row['body'])
            created_utc = row['created utc']
            score = row['score']
            subreddit = row['subreddit']
            
            parent_data = find_parent(parent_id)
